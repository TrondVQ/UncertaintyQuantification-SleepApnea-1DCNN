#!/usr/bin/env python3

import pandas as pd
import numpy as np
from sklearn.model_selection import GroupShuffleSplit
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
import argparse
import os
from typing import Dict, List, Any, Optional, Tuple

"""
This script prepares the final datasets for machine learning model training
and evaluation from a combined CSV file generated by a previous preprocessing step.

It performs the following key steps:
1. Loads the flattened, combined data (signals and annotations) from a CSV file.
2. Performs a patient-independent train/test split using GroupShuffleSplit
   to ensure no data from the same patient appears in both sets.
3. Reshapes the flattened 2D time-series data (samples, time_steps * features)
   into a 3D format suitable for sequence models (samples, time_steps, features).
4. Applies window-level standardization to the 3D data, standardizing each
   segment independently.
5. Applies balancing techniques:
   - SMOTE (Synthetic Minority Over-sampling Technique) to the training data
     to address class imbalance.
   - Random Under-Sampling (RUS) to the test data to create a balanced test set
     for evaluating model performance on a balanced distribution (in addition to the
     original unbalanced test set).
6. Saves the resulting 3D NumPy arrays for the unbalanced train, unbalanced test,
   and balanced test sets, along with their corresponding labels and test patient IDs.

Configuration paths and parameters can be adjusted via command-line arguments.
"""

# --- Configuration ---
# Define default paths and parameters (can be overridden by command-line args)

# Input CSV file from the initial preprocessing script
SHHS2_CSV_ALL: str = "./processed_shhs2_data.csv" # Update if your combined CSV has a different name or location

# Directory to save the final processed .npy datasets
OUTPUT_DIR: str = "./final_processed_datasets" # Changed output dir name for clarity

# Proportion of data to be allocated for the test set during splitting
TEST_SIZE: float = 0.20 # Corresponds to an 80/20 train/test split

# Random seed for reproducibility of splitting and balancing
RANDOM_SEED: int = 2025

# Define feature constants based on previous script output
ORIGINAL_FEATURES: List[str] = ["SaO2", "PR", "THOR RES", "ABDO RES"] # Ensure these match the columns from the previous script
NUM_FEATURES: int = len(ORIGINAL_FEATURES)
TIME_STEPS: int = 60 # Should match window size used in previous script

# Column names in the input CSV for the label and grouping
LABEL_COL: str = "Apnea/Hypopnea_Label" # Ensure this column name matches your CSV
GROUP_COL: str = "Patient_ID" # Ensure this column name matches your CSV

FEATURE_COLS: List[str] = [f"{col}_t{t}" for t in range(TIME_STEPS) for col in ORIGINAL_FEATURES]

def reshape_flat_to_3d(
        data_flat: np.ndarray,
        steps: int,
        features: int
) -> np.ndarray:
    """
    Reshapes flat data (samples, steps*features) to 3D (samples, steps, features).

    Args:
        data_flat: A 2D NumPy array with shape (num_samples, steps * features).
        steps: The number of time steps (window size).
        features: The number of features per time step.

    Returns:
        A 3D NumPy array with shape (num_samples, steps, features).

    Raises:
        ValueError: If the input data shape is incompatible with the target 3D shape.
    """
    num_samples = data_flat.shape[0]
    expected_flat_features = steps * features

    if data_flat.shape[1] != expected_flat_features:
        print(f"ERROR: Cannot reshape. Expected {expected_flat_features} flat features ({steps} * {features}), found {data_flat.shape[1]}.")
        raise ValueError("Incorrect number of features for reshaping.")

    try:
        # Reshape assuming the flat data is ordered [feat1_t0, feat1_t1,..., feat1_t59, feat2_t0,...]
        # This corresponds to reshaping a (samples, steps * features) array to (samples, steps, features)
        # where the last dimension groups features at each time step.
        reshaped = data_flat.reshape((num_samples, steps, features))
        print(f"Reshaped data from {data_flat.shape} to {reshaped.shape}")
        return reshaped
    except ValueError as e:
        print(f"ERROR during reshaping: {e}. Please check TIME_STEPS={steps}, NUM_FEATURES={features}, and the order of FEATURE_COLS / original flattening.")
        raise e

def standardize_per_window(
        data_3d: np.ndarray,
        epsilon: float = 1e-8
) -> np.ndarray:
    """
    Applies standardization independently to each window (sample) along the time axis.

    Args:
        data_3d: A 3D NumPy array with shape (num_samples, time_steps, features).
        epsilon: A small value added to the standard deviation to prevent division by zero.

    Returns:
        A 3D NumPy array with the same shape as input, containing standardized data.
    """
    print(f"Applying window-level standardization to data with shape {data_3d.shape}...")
    if data_3d.ndim != 3:
        print(f"Warning: Input data is not 3D ({data_3d.ndim} dimensions). Skipping window-level standardization.")
        return data_3d # Return original data if not 3D

    # Calculate mean and std along the time axis (axis=1) for each feature independently
    # Keepdims=True ensures the result shape allows for broadcasting: (n_samples, 1, n_features)
    mean = np.mean(data_3d, axis=1, keepdims=True)
    std = np.std(data_3d, axis=1, keepdims=True)

    # Apply standardization: (data - mean) / (std + epsilon)
    # Epsilon prevents division by zero for features that are constant within a window
    standardized_data = (data_3d - mean) / (std + epsilon)
    print("Window-level standardization applied.")
    return standardized_data

# --- Main Data Preparation Function ---

def prepare_final_datasets(
        input_csv: str,
        output_dir: str,
        test_size: float,
        seed: int
) -> None:
    """
    Loads flattened data from CSV, performs patient-independent splitting,
    reshapes to 3D, applies window-level standardization, applies balancing
    (SMOTE/RUS), and saves final 3D datasets as .npy files.

    Args:
        input_csv: Path to the combined processed CSV file.
        output_dir: Directory to save the final .npy datasets.
        test_size: Proportion of data for the test set split.
        seed: Random seed for reproducibility.
    """
    print(f"--- Starting Final Data Preparation (Window-Level Standardization) ---")
    print(f"Input CSV: {input_csv}")
    print(f"Output Directory: {output_dir}")
    print(f"Test Set Size: {test_size}")
    print(f"Random Seed: {seed}")

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # 1. Load the full dataset
    try:
        print("Loading data...")
        full_data = pd.read_csv(input_csv)
        print(f"Loaded data shape: {full_data.shape}")

        # Validate required columns are present
        required_cols = FEATURE_COLS + [LABEL_COL, GROUP_COL]
        if not all(col in full_data.columns for col in required_cols):
            missing = [col for col in required_cols if col not in full_data.columns]
            raise ValueError(f"Missing required columns in input CSV: {missing}")

    except FileNotFoundError:
        print(f"ERROR: Input CSV file not found at {input_csv}")
        return
    except Exception as e:
        print(f"ERROR: Failed to load or validate CSV: {e}")
        return

    # Handle potential NaN values in feature columns
    # It's often better to handle NaNs during initial preprocessing or more carefully here.
    # Filling with column means is a simple strategy.
    if full_data[FEATURE_COLS].isnull().values.any():
        print(f"NaN values found in feature columns. Filling with global column means.")
        # Calculate means only from non-NaN values
        means = full_data[FEATURE_COLS].mean()
        full_data[FEATURE_COLS] = full_data[FEATURE_COLS].fillna(means)

        # Verify if NaNs persist (e.g., if an entire column was NaN)
        if full_data[FEATURE_COLS].isnull().values.any():
            print(f"ERROR: NaN values persist after filling with means. Check data or use a different imputation method.")
            return

    # 2. Separate features, labels, and groups
    X_flat: pd.DataFrame = full_data[FEATURE_COLS]
    y: pd.Series = full_data[LABEL_COL]
    groups: pd.Series = full_data[GROUP_COL]

    # Basic check for sufficient data
    if X_flat.empty or y.empty or groups.empty:
        print("ERROR: No data found in the input CSV or essential columns are empty.")
        return

    # Check for minimum number of samples/groups for splitting
    if len(full_data) < 2 or len(groups.unique()) < 2:
        print("ERROR: Not enough data or patients for splitting. Need at least 2 samples and 2 unique patients.")
        return
    if test_size <= 0 or test_size >= 1:
        print(f"ERROR: Invalid test_size ({test_size}). Must be between 0 and 1.")
        return


    # 3. Patient-Independent Split (on flat data indices)
    print(f"\nPerforming patient-independent split (test_size={test_size}, seed={seed})...")
    # Use GroupShuffleSplit to ensure that all samples from a patient go into either the train or test set
    splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)

    try:
        # splitter.split returns indices. We take the first (and only) split.
        train_idx, test_idx = next(splitter.split(X_flat, y, groups))
    except ValueError as e:
        print(f"Error during splitting, potentially due to test_size or group distribution: {e}")
        print("Check if the number of unique groups is sufficient for the requested test_size.")
        return
    except Exception as e:
        print(f"An unexpected error occurred during splitting: {e}")
        return


    # Use .iloc to select rows based on indices
    X_train_flat: pd.DataFrame = X_flat.iloc[train_idx].copy() # Use .copy() to avoid SettingWithCopyWarning
    y_train: pd.Series = y.iloc[train_idx].copy()
    X_test_flat: pd.DataFrame = X_flat.iloc[test_idx].copy()
    y_test: pd.Series = y.iloc[test_idx].copy()
    groups_train: pd.Series = groups.iloc[train_idx].copy()
    groups_test: pd.Series = groups.iloc[test_idx].copy()


    print(f"Train shape (flat): {X_train_flat.shape}, Test shape (flat): {X_test_flat.shape}")
    train_patients = set(groups_train.unique())
    test_patients = set(groups_test.unique())

    # Verify no patient overlap between train and test sets
    if train_patients.intersection(test_patients):
        print("WARNING: Patient overlap detected between train and test sets!")
        # This should ideally not happen with GroupShuffleSplit if groups are unique patients
    else:
        print(f"Patient split verified: {len(train_patients)} unique patients in train, {len(test_patients)} in test. No overlap.")

    # 4. Reshape Data to 3D (Samples, TimeSteps, Features)
    print("\nReshaping data from 2D (flat) to 3D (samples, time_steps, features)...")
    try:
        # Convert pandas DataFrames to numpy arrays before reshaping
        X_train_3d: np.ndarray = reshape_flat_to_3d(X_train_flat.values, TIME_STEPS, NUM_FEATURES)
        X_test_3d: np.ndarray = reshape_flat_to_3d(X_test_flat.values, TIME_STEPS, NUM_FEATURES)
    except ValueError:
        # reshape_flat_to_3d already prints an error
        return

    # 5. Apply Window-Level Standardization
    # Standardize each time window independently for each feature.
    print("\nApplying window-level standardization...")
    X_train_std_win: np.ndarray = standardize_per_window(X_train_3d)
    X_test_std_win: np.ndarray = standardize_per_window(X_test_3d) # This is the final unbalanced test set features

    # --- Balancing Steps (Applied AFTER window-level standardization and splitting) ---

    # 6. Apply SMOTE (to Window-Standardized Training Data ONLY)
    # SMOTE requires 2D input (n_samples, n_features). The 3D data needs to be flattened.
    print("\nApplying SMOTE to window-standardized training data for balancing...")
    print(f"Original train class distribution:\n{y_train.value_counts()}") # Print raw counts too
    print(f"Original train class distribution (%):\n{y_train.value_counts(normalize=True)}")

    n_samples_train, steps, feats = X_train_std_win.shape
    # Flatten the 3D training data to 2D for SMOTE
    X_train_flat_for_smote: np.ndarray = X_train_std_win.reshape((n_samples_train, steps * feats))

    smote = SMOTE(random_state=seed, n_jobs=-1) # Use n_jobs=-1 for parallel processing if available
    try:
        # Apply SMOTE
        X_train_smote_flat: np.ndarray
        y_train_smote: np.ndarray
        X_train_smote_flat, y_train_smote = smote.fit_resample(X_train_flat_for_smote, y_train)

        print(f"SMOTE balanced train shape (flat): {X_train_smote_flat.shape}")
        print(f"SMOTE balanced train class distribution:\n{pd.Series(y_train_smote).value_counts()}") # Print raw counts
        print(f"SMOTE balanced train class distribution (%):\n{pd.Series(y_train_smote).value_counts(normalize=True)}")


        # Reshape the SMOTE-generated data back to 3D for saving
        X_train_smote_3d: np.ndarray = reshape_flat_to_3d(X_train_smote_flat, TIME_STEPS, NUM_FEATURES)

    except Exception as e:
        print(f"ERROR during SMOTE: {e}. Skipping SMOTE balancing.")
        # If SMOTE fails, use the original window-standardized training data
        print("Using original window-standardized training data (unbalanced) for training set.")
        X_train_smote_3d = X_train_std_win.copy() # Fallback to original
        y_train_smote = y_train.values.copy() # Fallback to original labels (as numpy array)


    # 7. Create Balanced Test Set using RUS (on Window-Standardized Test Data)
    # RUS also requires 2D input: flatten the window-standardized test data.
    print("\nApplying Random Under-Sampling (RUS) to window-standardized test data for a balanced test set...")
    print(f"Original test class distribution:\n{y_test.value_counts()}") # Print raw counts
    print(f"Original test class distribution (%):\n{y_test.value_counts(normalize=True)}")

    n_samples_test = X_test_std_win.shape[0]
    # Flatten the 3D test data to 2D for RUS
    X_test_flat_for_rus: np.ndarray = X_test_std_win.reshape((n_samples_test, steps * feats))

    rus = RandomUnderSampler(random_state=seed)
    X_test_rus_3d: Optional[np.ndarray] = None # Initialize as Optional, might remain None if RUS fails
    y_test_rus: Optional[np.ndarray] = None

    # Check if RUS is possible (need at least one sample in the minority class)
    if y_test.min() == y_test.max():
        print("Warning: Only one class present in original test set. Cannot perform RUS for balanced test set.")
    else:
        try:
            # Apply RUS
            X_test_rus_flat: np.ndarray
            y_test_rus_np: np.ndarray # Use temp variable for np array output from fit_resample
            X_test_rus_flat, y_test_rus_np = rus.fit_resample(X_test_flat_for_rus, y_test)
            y_test_rus = y_test_rus_np # Assign to Optional variable


            print(f"RUS balanced test shape (flat): {X_test_rus_flat.shape}")
            print(f"RUS balanced test class distribution:\n{pd.Series(y_test_rus).value_counts()}") # Print raw counts
            print(f"RUS balanced test class distribution (%):\n{pd.Series(y_test_rus).value_counts(normalize=True)}")

            # Reshape the RUS-generated data back to 3D for saving
            X_test_rus_3d = reshape_flat_to_3d(X_test_rus_flat, TIME_STEPS, NUM_FEATURES)

        except Exception as e:
            print(f"ERROR during RUS: {e}. Skipping RUS balancing for balanced test set.")
            # X_test_rus_3d and y_test_rus will remain None


    # 8. Save the processed 3D datasets as .npy files
    print(f"\nSaving processed 3D datasets to '{output_dir}'...")

    try:
        # --- Save files ---
        # Training set (Window Std + SMOTE)
        np.save(os.path.join(output_dir, 'X_train_win_std_smote.npy'), X_train_smote_3d)
        np.save(os.path.join(output_dir, 'y_train_smote.npy'), y_train_smote) # Labels from SMOTE

        # Unbalanced Test set (Window Std) - For realistic evaluation on original distribution
        np.save(os.path.join(output_dir, 'X_test_win_std_unbalanced.npy'), X_test_std_win)
        np.save(os.path.join(output_dir, 'y_test_unbalanced.npy'), y_test.values) # Original unbalanced labels
        # Save patient IDs for the unbalanced test set
        np.save(os.path.join(output_dir, 'patient_ids_test_unbalanced.npy'), groups_test.values)

        # Balanced Test set (Window Std + RUS), if RUS succeeded - For evaluating on balanced distribution
        if X_test_rus_3d is not None and y_test_rus is not None:
            np.save(os.path.join(output_dir, 'X_test_win_std_rus.npy'), X_test_rus_3d)
            np.save(os.path.join(output_dir, 'y_test_rus.npy'), y_test_rus)
            print("Saved RUS balanced test set.")
        else:
            print("Skipping save for RUS balanced test set as RUS failed or was skipped.")


        print("Datasets saved successfully.")
    except Exception as e:
        print(f"ERROR saving datasets: {e}")

    print(f"--- Final Data Preparation Finished ---")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Prepare final datasets for ML with window-level standardization and balancing."
    )
    parser.add_argument(
        "--input_csv",
        type=str,
        default=SHHS2_CSV_ALL,
        help=f"Path to the combined processed CSV file (default: {SHHS2_CSV_ALL})"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=OUTPUT_DIR,
        help=f"Directory to save the final .npy datasets (default: {OUTPUT_DIR})"
    )
    parser.add_argument(
        "--test_size",
        type=float,
        default=TEST_SIZE,
        help=f"Proportion of data for the test set (default: {TEST_SIZE})"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=RANDOM_SEED,
        help=f"Random seed for reproducibility (default: {RANDOM_SEED})"
    )

    args = parser.parse_args()

    # Ensure the default paths are used if not overridden by args
    input_csv_path = args.input_csv
    output_directory_path = args.output_dir

    # Note: The script will use the values from args even if they are the defaults

    prepare_final_datasets(
        input_csv=input_csv_path,
        output_dir=output_directory_path,
        test_size=args.test_size,
        seed=args.seed
    )